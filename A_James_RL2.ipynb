{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67675284",
   "metadata": {},
   "source": [
    "Here is my previous code for agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def run():\n",
    "    questions=['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    # Define the probabilities and rewards based on the inputA\n",
    "    probabilities = [0.9, 0.75, 0.5, 0.1]\n",
    "    rewards = [100, 1000, 10000, 50000]\n",
    "    total_reward=0\n",
    "    \n",
    "    for k in range(len(questions)):\n",
    "        Question = questions[k]\n",
    "        Decision=input('Do you want to play? Y/N')\n",
    "        while Decision not in ['Y','N']:\n",
    "            print('Wrong entry, please answer Y or N')\n",
    "            Decision=input('Do you want to play? Y/N')\n",
    "\n",
    "        if Decision == 'Y': #then we play\n",
    "            issue=random()\n",
    "            if issue<probabilities[k]: #then you won\n",
    "                print('You won the question',k+1)\n",
    "                rew = rewards[k]\n",
    "                total_reward+=rew\n",
    "            else : #then you lost\n",
    "                print('You lost')\n",
    "                break\n",
    "        else:\n",
    "            print('Quit')\n",
    "            break\n",
    "    print('Your total rewards is',total_reward)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a21dd",
   "metadata": {},
   "source": [
    "I did something really idiomatic firstly but lost a lot of time :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3813d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def getNextstates(state,action):\n",
    "    if state=='Q1':\n",
    "        return(['Q1_win','Q1_fail'])\n",
    "    if state=='Q2':\n",
    "        if action=='play':\n",
    "            return(['Q2_win','Q2_fail'])\n",
    "        else :\n",
    "            return(['quit'])\n",
    "    if state=='Q3':\n",
    "        if action=='play':\n",
    "            return(['Q3_win','Q3_fail'])\n",
    "        else:\n",
    "            return(['quit'])\n",
    "    if state=='Q4':\n",
    "        if action=='play':\n",
    "            return(['Q4_win','Q4_fail'])\n",
    "        else :\n",
    "            return(['quit'])\n",
    "\n",
    "\n",
    "def getProbTr(state,next_state):\n",
    "    trans_probabilities = [0.9, 0.75, 0.5, 0.1]\n",
    "    if state=='Q1':\n",
    "        if next_state=='Q1_win':\n",
    "            return(0.9)\n",
    "        elif next_state=='Q1_fail':\n",
    "            return(0.1)\n",
    "    if state=='Q2':\n",
    "        if next_state=='Q2_win':\n",
    "            return(0.75)\n",
    "        elif next_state=='Q2_fail':\n",
    "            return(0.25)\n",
    "        elif next_state=='quit':\n",
    "            return(1.0)\n",
    "    if state=='Q3':\n",
    "        if next_state=='Q3_win':\n",
    "            return(0.5)\n",
    "        elif next_state=='Q3_fail':\n",
    "            return(0.5)\n",
    "        elif next_state=='quit':\n",
    "            return(1.0)\n",
    "    if state=='Q4':\n",
    "        if next_state=='Q4_win':\n",
    "            return(0.9)\n",
    "        elif next_state=='Q4_fail':\n",
    "            return(0.1)\n",
    "        elif next_state=='quit':\n",
    "            return(1.0)\n",
    "\n",
    "def getRewardVal(state,next_state):\n",
    "    if state=='Q1':\n",
    "        if next_state=='Q1_win':\n",
    "            return(100)\n",
    "        else: #you either fail or quit\n",
    "            return(0)\n",
    "    if state=='Q2':\n",
    "        if next_state=='Q2_win':\n",
    "            return(1100)\n",
    "        else: #you either fail or quit\n",
    "            return(100)\n",
    "    if state=='Q3':\n",
    "        if next_state=='Q3_win':\n",
    "            return(11100)\n",
    "        else: #you either fail or quit\n",
    "            return(1100)\n",
    "    if state=='Q4':\n",
    "        if next_state=='Q4_win':\n",
    "            return(61100)\n",
    "        else: #you either fail or quit\n",
    "            return(11100)\n",
    "        \n",
    "def getMaxVal(next_state):\n",
    "    \n",
    "    return(1) #for now\n",
    "  \n",
    "\n",
    "def best_policy():\n",
    "    states=['Q1','Q2', 'Q3', 'Q4']\n",
    "    actions=['quit','play']\n",
    "    rewards = [100, 1000, 10000, 50000]\n",
    "    trans_probabilities = [0.9, 0.75, 0.5, 0.1]\n",
    "    q_matrix=np.zeros((4,2))\n",
    "    print(q_matrix)\n",
    "    convergence=False\n",
    "    gamma=0.99\n",
    "    while not convergence :\n",
    "        new_q=np.zeros((4,2))\n",
    "        for state in states:\n",
    "            raw_idx=states.index(state)\n",
    "            for action in actions:\n",
    "                col_idx=actions.index(action)\n",
    "                next_states=getNextstates(state,action)\n",
    "                sum1=0\n",
    "                for k in range(len(next_states)):\n",
    "                    prob= getProbTr(state,next_states[k])\n",
    "                    reward = getRewardVal(state,next_states[k])\n",
    "                    max_val=getMaxVal(next_states[k])\n",
    "                    sum1+=prob*(reward+gamma*max_val)\n",
    "                new_q[raw_idx,col_idx] =sum1\n",
    "        \n",
    "        diff=0\n",
    "        for state in states:\n",
    "            raw_idx=states.index(state)\n",
    "            for action in actions:\n",
    "                col_idx=actions.index(action)\n",
    "                diff+=abs(new_q[raw_idx,col_idx]-q_matrix[raw_idx,col_idx])\n",
    "        if diff<0.5 :\n",
    "            print('convergence')\n",
    "            return(new_q)\n",
    "            \n",
    "        else :\n",
    "            q_matrix=new_q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f20b5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "convergence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   90.99,    90.99],\n",
       "       [  100.99,   850.99],\n",
       "       [ 1100.99,  6100.99],\n",
       "       [11100.99, 56100.99]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2806d60",
   "metadata": {},
   "source": [
    "and I rewrote it using matrices, but obviously for the last state I can't compute the next state s' so my last raw of Q is empty: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5224b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence\n",
      "[[25937.8375 17557.6125]\n",
      " [14455.     13955.    ]\n",
      " [ 5000.      9000.    ]\n",
      " [    0.         0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration(states,actions,gamma):\n",
    "    R = np.array([    #reward matrix\n",
    "        [100, 0],\n",
    "        [1000, 100],\n",
    "        [10000, 1000],\n",
    "        [50000,10000]])\n",
    "    P=np.array([       #it's the probabilities of transitions\n",
    "        [0.9,0.1],\n",
    "        [0.75,0.25],\n",
    "        [0.5,0.5],\n",
    "        [0.1,0.9]])\n",
    "    Q = np.zeros((states,actions)) #this is the Q table\n",
    "\n",
    "    convergence=False\n",
    "    step=0\n",
    "    while not convergence:\n",
    "        last_Q = np.copy(Q) #last state of the Q matrix\n",
    "        for state in range(states):   #to fill the Q matrix\n",
    "            for action in range(actions):   #to fill the Q matrix\n",
    "                sum1=0\n",
    "                for state_prime in range(state+1,states):  #sum over the s'\n",
    "                    best_action=np.zeros(actions)\n",
    "                    for k in range(actions):  #looking for the argmax over a'\n",
    "                        best_action[k]=last_Q[state_prime,k]\n",
    "                    best=max(best_action)\n",
    "                    sum1+=P[state_prime,action]*(R[state_prime,action]+gamma*best)\n",
    "                Q[state,action]=sum1\n",
    "        diff=0\n",
    "        for action in range(actions):\n",
    "            for state in range(states):\n",
    "                diff+=abs(Q[state,action]-last_Q[state,action])\n",
    "        if diff<0.01 :\n",
    "            print('convergence')\n",
    "            convergence=True\n",
    "        else:\n",
    "            step+=1\n",
    "    print(Q)\n",
    "value_iteration(states=4,actions=2,gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04699021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
